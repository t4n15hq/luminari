{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/t4n15hq/luminari/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ka9Pq10hbGj",
        "outputId": "abf4ac4c-1132-4abf-9b2a-8a3b1534535e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import efficientnet\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, Model, optimizers\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JoKy7N0oRTR1"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.base_path = '/content/drive/MyDrive/dataset/SkinDisease'\n",
        "        self.processed_dir = os.path.join(self.base_path, 'processed_data')\n",
        "        self.model_dir = os.path.join(self.base_path, 'models')\n",
        "        self.image_size = (224, 224, 3)  # Increased image size\n",
        "        self.batch_size = 8  # Reduced for gradient stability\n",
        "        self.epochs = 150  # Increased training time\n",
        "        self.initial_learning_rate = 5e-5  # Adjusted learning rate\n",
        "        self.validation_split = 0.2\n",
        "        self.learning_rate = 0.0001  # Added learning rate\n",
        "        self.use_mixed_precision = True  # Enable mixed precision training\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(self.model_dir, exist_ok=True)\n",
        "\n",
        "        # Enable mixed precision\n",
        "        if self.use_mixed_precision:\n",
        "            policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
        "            tf.keras.mixed_precision.set_global_policy(policy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def normalize_image(self, image):\n",
        "        \"\"\"Normalize image with contrast enhancement\"\"\"\n",
        "        mean = np.mean(image)\n",
        "        std = np.std(image)\n",
        "        normalized = (image - mean) / (std + 1e-7)\n",
        "        return np.clip(normalized, -3, 3)  # Clip outliers\n",
        "\n",
        "    def load_data(self, data_type='train_balanced'):\n",
        "        data_dir = os.path.join(self.config.processed_dir, data_type)\n",
        "        print(f\"Loading data from: {data_dir}\")\n",
        "\n",
        "        if not os.path.exists(data_dir):\n",
        "            raise Exception(f\"Data directory not found: {data_dir}\")\n",
        "\n",
        "        images = []\n",
        "        labels = []\n",
        "        class_names = []\n",
        "\n",
        "        # Changed from .npy to .npz\n",
        "        class_files = sorted([f for f in os.listdir(data_dir) if f.endswith('.npz')])\n",
        "        print(f\"Found {len(class_files)} classes\")\n",
        "\n",
        "        # Load and preprocess data\n",
        "        for i, class_file in enumerate(tqdm(class_files, desc=\"Loading classes\")):\n",
        "            class_name = class_file.replace('.npz', '')  # Changed from .npy to .npz\n",
        "            class_names.append(class_name)\n",
        "\n",
        "            # Load from npz file using 'data' key\n",
        "            loaded_data = np.load(os.path.join(data_dir, class_file))\n",
        "            class_images = loaded_data['data']\n",
        "\n",
        "            # Apply normalization to each image\n",
        "            class_images = np.array([self.normalize_image(img) for img in class_images])\n",
        "\n",
        "            images.append(class_images)\n",
        "            labels.extend([i] * len(class_images))\n",
        "\n",
        "        images = np.concatenate(images)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        # Calculate class weights\n",
        "        class_weights = compute_class_weight('balanced',\n",
        "                                           classes=np.unique(labels),\n",
        "                                           y=labels)\n",
        "        class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "        print(f\"Loaded {len(images)} images with shape {images.shape}\")\n",
        "        return images, labels, len(class_files), class_weight_dict, class_names\n",
        "\n",
        "    def analyze_class_distribution(self):\n",
        "        \"\"\"Analyze and visualize class distribution\"\"\"\n",
        "        data_dir = os.path.join(self.config.processed_dir, 'train_balanced')\n",
        "        distribution = {}\n",
        "\n",
        "        # Changed from .npy to .npz\n",
        "        for class_file in os.listdir(data_dir):\n",
        "            if class_file.endswith('.npz'):  # Changed extension\n",
        "                class_name = class_file.replace('.npz', '')  # Changed extension\n",
        "                loaded_data = np.load(os.path.join(data_dir, class_file))\n",
        "                class_data = loaded_data['data']  # Access using 'data' key\n",
        "                distribution[class_name] = len(class_data)\n",
        "\n",
        "        # Plot distribution\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.bar(distribution.keys(), distribution.values())\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.title('Class Distribution')\n",
        "        plt.ylabel('Number of Images')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return distribution"
      ],
      "metadata": {
        "id": "CW3WhEX_O1VM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup(x, y, alpha=0.2):\n",
        "    \"\"\"Performs MixUp augmentation\"\"\"\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    index = tf.random.shuffle(tf.range(batch_size))\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * tf.gather(x, index)\n",
        "    mixed_y = lam * y + (1 - lam) * tf.gather(y, index)\n",
        "    return mixed_x, mixed_y"
      ],
      "metadata": {
        "id": "1n4t3PMwO5sW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "K7jp8_RYRXo4"
      },
      "outputs": [],
      "source": [
        "class ModelBuilder:\n",
        "    def __init__(self, config, num_classes):\n",
        "        self.config = config\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def build_model(self):\n",
        "        # Using EfficientNetB4\n",
        "        base_model = tf.keras.applications.EfficientNetB4(\n",
        "            weights='imagenet',\n",
        "            include_top=False,\n",
        "            input_shape=(224, 224, 3)\n",
        "        )\n",
        "\n",
        "        base_model.trainable = False\n",
        "\n",
        "        inputs = tf.keras.Input(shape=(224, 224, 3))\n",
        "        x = tf.keras.applications.efficientnet.preprocess_input(inputs)\n",
        "        x = base_model(x, training=False)\n",
        "        x = layers.GlobalAveragePooling2D()(x)  # Output shape: (None, 1792)\n",
        "\n",
        "        # Store initial features for residual connection with matching shape\n",
        "        initial_features = layers.Dense(2048)(x)  # Match the shape of next dense layer\n",
        "\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "\n",
        "        # First dense block\n",
        "        x = layers.Dense(2048,\n",
        "                        activation='relu',\n",
        "                        kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.5)(x)\n",
        "\n",
        "        # Add residual connection (now shapes match: both 2048)\n",
        "        x = layers.Add()([x, initial_features])\n",
        "        x = layers.Activation('relu')(x)\n",
        "\n",
        "        # Second dense block\n",
        "        x = layers.Dense(1024, activation='relu',\n",
        "                        kernel_regularizer=tf.keras.regularizers.l2(0.01))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Dropout(0.4)(x)\n",
        "\n",
        "        outputs = layers.Dense(self.num_classes, activation='softmax')(x)\n",
        "\n",
        "        model = Model(inputs, outputs)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizers.Adam(learning_rate=self.config.learning_rate),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    def unfreeze_and_recompile(self, model, num_layers_to_unfreeze=15):\n",
        "        base_model = model.get_layer('efficientnetb4')  # Changed to efficientnetb4\n",
        "        base_model.trainable = True\n",
        "\n",
        "        # Freeze earlier layers\n",
        "        for layer in base_model.layers[:-num_layers_to_unfreeze]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        # Lower learning rate for fine-tuning\n",
        "        model.compile(\n",
        "            optimizer=optimizers.Adam(learning_rate=5e-6),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5DGYouFIhytw"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "    def train(self, model, train_data, train_labels, val_data, val_labels, class_weights):\n",
        "        # Enhanced data augmentation\n",
        "        train_datagen = ImageDataGenerator(\n",
        "            rotation_range=20,\n",
        "            width_shift_range=0.2,\n",
        "            height_shift_range=0.2,\n",
        "            horizontal_flip=True,\n",
        "            vertical_flip=False,\n",
        "            brightness_range=[0.7, 1.3],\n",
        "            zoom_range=0.2,\n",
        "            shear_range=0.15,\n",
        "            channel_shift_range=20.0,\n",
        "            fill_mode='nearest'\n",
        "        )\n",
        "\n",
        "        # Setup TensorBoard logging\n",
        "        log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "        tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "            log_dir=log_dir, histogram_freq=1\n",
        "        )\n",
        "\n",
        "        callbacks = [\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                filepath=os.path.join(self.config.model_dir, 'best_model.h5'),\n",
        "                save_best_only=True,\n",
        "                monitor='val_accuracy',\n",
        "                mode='max'\n",
        "            ),\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=15,\n",
        "                restore_best_weights=True\n",
        "            ),\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.2,\n",
        "                patience=7,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "            tensorboard_callback,\n",
        "            tf.keras.callbacks.CSVLogger('training_log.csv')\n",
        "        ]\n",
        "\n",
        "        history = model.fit(\n",
        "            train_datagen.flow(\n",
        "                train_data, train_labels,\n",
        "                batch_size=self.config.batch_size\n",
        "            ),\n",
        "            validation_data=(val_data, val_labels),\n",
        "            epochs=self.config.epochs,\n",
        "            callbacks=callbacks,\n",
        "            class_weight=class_weights\n",
        "        )\n",
        "\n",
        "        return history\n",
        "\n",
        "    def plot_training_history(self, history):\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Plot accuracy\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(history.history['accuracy'])\n",
        "        plt.plot(history.history['val_accuracy'])\n",
        "        plt.title('Model Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend(['Train', 'Validation'])\n",
        "\n",
        "        # Plot loss\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(history.history['loss'])\n",
        "        plt.plot(history.history['val_loss'])\n",
        "        plt.title('Model Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend(['Train', 'Validation'])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceJjb0pshzgf",
        "outputId": "2c8dcf05-8326-44cf-a94e-0b195ed8d4fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting improved training pipeline...\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    try:\n",
        "        print(\"Starting improved training pipeline...\")\n",
        "\n",
        "        # Initialize configuration\n",
        "        config = Config()\n",
        "\n",
        "        # Load and analyze data\n",
        "        data_loader = DataLoader(config)\n",
        "        data_loader.analyze_class_distribution()  # Visualize class distribution\n",
        "\n",
        "        # Load data with class weights\n",
        "        train_images, train_labels, num_classes, class_weights, class_names = data_loader.load_data('train_balanced')\n",
        "        test_images, test_labels, _, _, _ = data_loader.load_data('test')\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            train_images, train_labels,\n",
        "            test_size=config.validation_split,\n",
        "            random_state=42,\n",
        "            stratify=train_labels\n",
        "        )\n",
        "\n",
        "        print(f\"\\nData shapes:\")\n",
        "        print(f\"Training: {X_train.shape}\")\n",
        "        print(f\"Validation: {X_val.shape}\")\n",
        "        print(f\"Test: {test_images.shape}\")\n",
        "\n",
        "        # Build and train model\n",
        "        model_builder = ModelBuilder(config, num_classes)\n",
        "        model = model_builder.build_model()\n",
        "        model.summary()\n",
        "\n",
        "        trainer = Trainer(config)\n",
        "\n",
        "        # Phase 1: Training with frozen backbone\n",
        "        print(\"\\nPhase 1: Training with frozen backbone...\")\n",
        "        history1 = trainer.train(model, X_train, y_train, X_val, y_val, class_weights)\n",
        "        trainer.plot_training_history(history1)\n",
        "\n",
        "        # Phase 2: Fine-tuning\n",
        "        print(\"\\nPhase 2: Fine-tuning...\")\n",
        "        model = model_builder.unfreeze_and_recompile(model)\n",
        "        history2 = trainer.train(model, X_train, y_train, X_val, y_val, class_weights)\n",
        "        trainer.plot_training_history(history2)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
        "        print(f\"\\nFinal Test accuracy: {test_accuracy:.4f}\")\n",
        "        print(f\"Final Test loss: {test_loss:.4f}\")\n",
        "\n",
        "        # Save final model\n",
        "        model.save(os.path.join(config.model_dir, 'final_model.h5'))\n",
        "\n",
        "        # Save class names\n",
        "        with open(os.path.join(config.model_dir, 'class_names.txt'), 'w') as f:\n",
        "            for class_name in class_names:\n",
        "                f.write(f\"{class_name}\\n\")\n",
        "\n",
        "        print(\"\\nTraining completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError in training pipeline: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPxYpSdWHdUFLaQhgSjz+Rt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}